# Lexical Analyzer

This project is a Python-based lexical analyzer that tokenizes input text according to predefined rules. It's designed to be a simple tool for breaking down input text into tokens, which can then be used for further analysis or processing.

## Features

- Tokenization of input text based on specified rules.
- Support for recognizing various token types such as identifiers, keywords, operators, etc.
- Flexible configuration for defining custom tokenization rules.
- Command-line interface for easy usage and integration into other tools.

## Installation

1. Clone the repository:

    ```bash
    git clone https://github.com/pranesh0512/jsonParser.git
    ```

2. Navigate to the project directory:

    ```bash
    cd parser
    ```



## Usage

### Basic Usage

To tokenize input text using the default configuration, simply run the following command:

```bash
python main.py <input_file>
